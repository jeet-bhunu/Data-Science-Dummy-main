{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"},"cell_type":"markdown","source":"# Automated feature selection with sklearn\n\n## Discussion\n\nThe optimal machine learning problem approach is to take a dataset, perform extensive EDA on it, and understand many to most of the important properties of the predictors _before_ getting as far as seriously training models on these variables. However, this is not always possible. Sometimes the dataset has too many variables. Datasets may easily have hundreds or even thousands of variables, quickly outrunning human comprehension. Other times there just isn't enough time. Many machine learning -related Kaggle notebooks, for example, are based on easily processed data, or only on well-understood subsets of data. This is due to laziness, as there is a boundary to how many variables your average notebook author is willing to deal with, and how much time they're willing to dedicate to the task at hand.\n\n**Feature selection** is the process of tuning down the number of predictor variables used by the models you build. For example, when faced with two models with the same or nearly the same score, but with the latter model using more variables, your immediate instinct should be to choose the one with fewer variables. That model is simpler to train, simpler to understand, easier to run, and less likely to be leaky. Tuning the number of parameters in a model is a natural part of data science in practice, and is something that comes naturally as part of the model-building process. While the number of features is small or you have time to sit down and consider them all feature selection is mainly a hand-driven process. In scenarios where the number of variables are overwhelming, or your time is limited, automated or semi-automated feature selection can speed things up.\n\nAnd even when you do have the incentive to hand-roll and -curate your features, automated feature selection provides some useful early directions for exploration during the exploratory process.\n\nIn this notebook I examine and demonstrate some tools available in Python for performing automated feature selection. I find almost all the `sklearn` feature selection algorithms to be useful (if not to use, then at least to think about), so this notebook has very similar coverage to the material covered in the [official documentation](http://scikit-learn.org/stable/modules/feature_selection.html)."},{"metadata":{"_uuid":"db4cc9cd750c861bcbab7df3c7dcd66be77e380b","_cell_guid":"d1594b2f-e5c6-479e-968a-9110452f2e9d","collapsed":true,"trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true},"cell_type":"markdown","source":"## GenericUnivariateSelect\n\n`GenericUnivariateSelect` is a `sklearn.feature_selection` tool that allows you to select features from a dataset using a scoring function. It supports selecting columns in one of a few different configurations: `k` for when you want a specific number of columns, `percentile` for when you want to a percentage of the total number of columns, and so on.\n\nThis operator's main feature is its lack of opinion. Defining the metric is up to you; you pass it a function and it does the rest.\n\nTo demonstrate the API let's first try `GenericUnivariateSelect` out on a simple character recognition problem."},{"metadata":{"_uuid":"530009d340810abce6d643d2a32e8d4adf66e22e","_cell_guid":"c7c5456d-def1-4400-b2ff-66a120cb1105","collapsed":true,"trusted":false},"cell_type":"code","source":"chars = pd.concat([pd.read_csv(\"../input/ahcd1/csvTestImages 3360x1024.csv\", header=None), pd.read_csv(\"../input/ahcd1/csvTrainImages 13440x1024.csv\", header=None)])\nchars = chars.assign(label=pd.concat([pd.read_csv(\"../input/ahcd1/csvTestLabel 3360x1.csv\", header=None), pd.read_csv(\"../input/ahcd1/csvTrainLabel 13440x1.csv\", header=None)]).values)\n\nchars_X = chars.iloc[:, :-1]\nchars_y = chars.iloc[:, -1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2438791007c6130eacce4d3af0fa133d081612f3","_cell_guid":"9231544d-2523-4a24-a3be-9405b9ad72ed"},"cell_type":"markdown","source":"This dataset consists of several thousand 32-by-32 pixel images of handwritten Arabic characters. Here's one example of a glyph from the dataset:"},{"metadata":{"_uuid":"45e1fa95d09ceda271c0765dca995279a0eb92e5","_cell_guid":"e948522a-ca52-489f-aa5a-31f7ba0b77fd","trusted":false,"collapsed":true},"cell_type":"code","source":"sns.heatmap(chars.iloc[100, :-1].values.reshape((32, 32)).T, cmap='Greys', cbar=False)\nplt.axis('off')\npass","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52523f8fbbe3e1bac6c6f19a2a4439880ac15cfe","_cell_guid":"d7c89b32-0a55-4c6a-9dc1-1c4b22e0a7a5"},"cell_type":"markdown","source":"In training on this dataset it may naturally occur to us that, hey, most of the space in the image is actually whitespace: the space *around* the character not taken up by the character itself. Given this fact, we might decide that a worthwhile strategy would be to first preprocess the data to remove the pixel values that are mostly whitespace. Here is a heatmap demonstrating what implementing this strategy would look like:"},{"metadata":{"_uuid":"542e993b682ea9207076e13ead1f1cd39720bc64","_cell_guid":"88c166cb-0e16-4080-9ae1-a288557e5c7a","trusted":false,"collapsed":true},"cell_type":"code","source":"def sp(p, ax):\n    char_pixel_means = chars_X.mean(axis=0).values\n    idx_selected = np.where(char_pixel_means > np.percentile(char_pixel_means, p))[0]\n    arr = np.isin(np.array(list(range(32*32))), idx_selected).reshape((32, 32)).astype(int) == False\n    sns.heatmap(chars_X.mean(axis=0).values.reshape(32, 32), ax=ax, cbar=False, mask=arr)\n    ax.set_aspect('equal')\n    ax.axis('off')\n    ax.set_title(\"{0}% Selected\".format(100-p))\n\nfig, axarr = plt.subplots(1, 6, figsize=(12, 3))\nsns.heatmap(chars_X.mean(axis=0).values.reshape(32, 32), ax=axarr[0], cbar=False)\naxarr[0].axis('off')\naxarr[0].set_aspect('equal')\naxarr[0].set_title(\"100% Selected\")\nplt.suptitle(\"Feature Curation by Mean Pixel Value\", fontsize=16)\n\nsp(50, axarr[1])\nsp(60, axarr[2])\nsp(70, axarr[3])\nsp(80, axarr[4])\nsp(90, axarr[5])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ff95438cb94c663ebae0cd9bd819d983828707f","_cell_guid":"d166580f-fefe-4469-b98c-e557767c5cb6"},"cell_type":"markdown","source":"This heatmap shows the mean value of each pixel in the dataset. White-hot pixels are the ones that most often have data (a glyph passes through them), while black pixels lack any data. As we go further right we retain fewer and fewer of the original pixels.\n\nBy looking at this point you might reasonably conclude that curating the feature space by 50% would be appropriate. `GenericUnivariateSelect` makes it easy to implement this strategy. Here's all you need to do:"},{"metadata":{"_uuid":"74afa253f08f85d7e271576ee3a21bfbc0a5a3eb","_cell_guid":"5f05f695-b4a6-436a-8168-33f304b4e236","collapsed":true,"trusted":false},"cell_type":"code","source":"from sklearn.feature_selection import GenericUnivariateSelect\n\ntrans = GenericUnivariateSelect(score_func=lambda X, y: X.mean(axis=0), mode='percentile', param=50)\nchars_X_trans = trans.fit_transform(chars_X, chars_y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ec5e8dd5dff8d306f76e5f2ca9cf8ecbc893d4a","_cell_guid":"4707f995-5088-4e62-8b50-a2f235d888ff"},"cell_type":"markdown","source":"Using this function revolves around inputting a function that takes the `X` and `y` arrays, performs some kind of statistical test on the values, and then returns the score per feature in `X`."},{"metadata":{"_uuid":"d42b0351702101b31dd226473bd3caf13a35c434","_cell_guid":"7afb4bf6-2bef-462f-9b05-9d10bb6ab0f9","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"We started with {0} pixels but retained only {1} of them!\".format(chars_X.shape[1], chars_X_trans.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a0247daf3eae27b9f2dc71315166e633b05d5ff","_cell_guid":"89f9e2c5-4280-4d1e-bf6a-b628db913d30"},"cell_type":"markdown","source":"It's really useful to able to hand-roll your metrics like this. But for automated feature selection, we probably want to avoid coming up with our own expressions. Luckily `sklearn` defines a handful of different pre-built statistical tests that we may use. These are:\n\n* For regression: `f_regression`, `mutual_info_regression`\n* For classification: `chi2`, `f_classif`, `mutual_info_classif`\n\nAll of these tests in some way attempt to infer the \"relatedness\" of the predictor variables with the target variables. There are two classes of these tests, one implemented for regression and the other implemented for classification.\n\nI prefer the `mutual_info_*` tests because they \"just work\" out of the box. These are non-parametric tests which use k-nearest neighbors to measure the scale of the relationship between the predictor values and the target variable. The F-test statistics only find *linear* relationships, making them only appropriate for performing linear regression, and the chi-squared test requires that the data be appropriately scaled (which it rarely is) and non-negative (which it only sometimes is).\n\nTo demonstrate, I'll use the Kepler dataset. This dataset contains information on potential planets observed and confirmed or unconfirmed by the Kepler Space Observatory."},{"metadata":{"_uuid":"02f93e4a005f7cfd74cb176da4b82e4245604caf","_cell_guid":"4fe71c57-7c40-44be-8069-994a9a32f170","trusted":false,"collapsed":true},"cell_type":"code","source":"pd.set_option('max_columns', None)\nkepler = pd.read_csv(\"../input/kepler-exoplanet-search-results/cumulative.csv\")\nkepler = (kepler\n     .drop(['rowid', 'kepid'], axis='columns')\n     .rename(columns={'koi_disposition': 'disposition', 'koi_pdisposition': 'predisposition'})\n     .pipe(lambda df: df.assign(disposition=(df.disposition == 'CONFIRMED').astype(int), predisposition=(df.predisposition == 'CANDIDATE').astype(int)))\n     .pipe(lambda df: df.loc[:, df.dtypes.values != np.dtype('O')])  # drop str columns\n     .pipe(lambda df: df.loc[:, (df.isnull().sum(axis='rows') < 500).where(lambda v: v).dropna().index.values])  # drop columns with greater than 500 null values\n     .dropna()\n)\n\nkepler_X = kepler.iloc[:, 1:]\nkepler_y = kepler.iloc[:, 0]\n\nkepler.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a198756117bd1a662a9f4c77d4eca1bc60def749","_cell_guid":"8577c071-f1e6-47ad-ad7f-f7061918d044"},"cell_type":"markdown","source":"We can apply `mutual_info_classif` by running a simple built-in. Here I've plotted out the resulting per-variable score in a heatmap."},{"metadata":{"_uuid":"5c6550751f8c65319be0ce22b0e67cc4fae34106","_cell_guid":"dc77caf8-dd28-4e3e-85e1-3660c29cf19e","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_classif\nkepler_mutual_information = mutual_info_classif(kepler_X, kepler_y)\n\nplt.subplots(1, figsize=(26, 1))\nsns.heatmap(kepler_mutual_information[:, np.newaxis].T, cmap='Blues', cbar=False, linewidths=1, annot=True)\nplt.yticks([], [])\nplt.gca().set_xticklabels(kepler.columns[1:], rotation=45, ha='right', fontsize=12)\nplt.suptitle(\"Kepler Variable Importance (mutual_info_classif)\", fontsize=18, y=1.2)\nplt.gcf().subplots_adjust(wspace=0.2)\npass","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d16960cb87df888d0ccdf9d8fe0662d0c9bcc3e","_cell_guid":"9824da33-38a6-456e-9044-063dfd3708ad"},"cell_type":"markdown","source":"Unsurprisingly the variable that best predicts whether or not Kepler will classify an object as a planet or not is the `predisposition`&mdash;that is, whether or not the planet is already considered a `CANDIDATE` or not (usually the alternative is `FALSE POSITIVE`, and Kepler is just double-checking).\n\nSuppose we want to work with just the top fifty percent of the columns here. Then we would do:"},{"metadata":{"_uuid":"c95222407bf1c86eac7c48f9f60036e516c61af0","_cell_guid":"7ac90318-49af-4a0f-81ac-1ab2b836d489","trusted":false,"collapsed":true},"cell_type":"code","source":"trans = GenericUnivariateSelect(score_func=mutual_info_classif, mode='percentile', param=50)\nkepler_X_trans = trans.fit_transform(kepler_X, kepler_y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ecb50bc01960cf407c8903945577da31251a280","_cell_guid":"94ba53fc-b061-4000-8d8b-3d6417486249","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"We started with {0} features but retained only {1} of them!\".format(kepler_X.shape[1] - 1, kepler_X_trans.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"489e6729e52ae337e56f888c00037489ea2d3f50","_cell_guid":"1ce137b1-d9ec-4477-851f-1b8330d04cd0"},"cell_type":"markdown","source":"So far we've been using `GenericUnivariateSelect` with `percentile` arguments, however the available selection options are actually `{‘percentile’, ‘k_best’, ‘fpr’, ‘fdr’, ‘fwe’}`. The second of these is obviously \"give me the N best features\", but the remaining three options take a bit of explaining. All three of them are statistical significance tests, e.g. they are designed to select all columns which pass a certain p-value threshold. `fpr` is the false positive rate: we may use this argument to select columns with a certain level of risk (e.g. 25%) that they not correlated after all. `fdr` is the false discovery rate, and can be specified to select columns with a certain level of risk that actually correlated columns will be rejected. Finally, `fwe` is family-wide error rate; this can be used to control the level of risk that *at least one* of the columns returned is not actually correlated.\n\nThat's a lot of statistical trickery. Up to you whether or not you're comfortable using this toolchain.\n\nHere are the columns that we selected:"},{"metadata":{"_uuid":"230f808ede54a5826c435eb023656f4b5dd301df","_cell_guid":"3e9ca8b3-de14-4ea3-92af-6bf01f2f6072","trusted":false,"collapsed":true},"cell_type":"code","source":"columns_retained_Select = kepler.iloc[:, 1:].columns[trans.get_support()].values\npd.DataFrame(kepler_X_trans, columns=columns_retained_Select).head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd1a85ac18cb0468cef510a91c24b3a78deec7e0","_cell_guid":"320ffa5d-259d-4284-93dc-4790325bddb0"},"cell_type":"markdown","source":"Note the use of the `get_support()` method on the `GenericUnivariateSelect` object to get the indices of the columns that were carried over.\n\nFor the official documentation on `GenericUnivariateSelect` click [here](http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)."},{"metadata":{"_uuid":"c37624e84caaa1d75feebdb50ff6c6950bb0e217","_cell_guid":"fce34aa8-4999-4303-86a6-70a9633ee2b8"},"cell_type":"markdown","source":"## Model-based feature selection (SelectFromModel)\n\nSome machine learning algorithms naturally assign importance to dataset features in some way. The obvious example is linear regression, which works by applying a coefficient multiplier to each of the features. Obviously, the higher the coefficient, the more valuable the feature. Other good examples are lasso regression, which features built-in variable selection (I show an application of this feature [here](https://www.kaggle.com/residentmario/lasso-regression-with-tennis-odds/)) by taking the `coef_` of features deemed insignificant to 0, and decision trees, which feature expose Gini importance via a `feature_importances_` class variable.\n\nWe can perform feature selection by using the feature ranks generated by such machine learning models as a ranking, and then pruning the features based on that ranking. This approach is good because it is non-parametric; basing our selection on features that get picked by an *actual* machine learning algorithm, not a statistical test, makes a lot of sense. Of course, in this case the choice of the algorithm you use matters a lot. The `sklearn` documentation describing this approach ([here](http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)) points out the best options for this approach: `linear_model.Lasso` or `trees.DecisionTreeRegressor` for regression, and one of `linear_model.LogisticRegression` and `svm.LinearSVC` and `trees.DecisionTreeClassifier` for classification.\n\nIf your goal is to build a logistic model or a linear support vector machine then it makes sense to make your feature selection decisions based on outputs from those two models, but in general decision trees are by far the most popular choice for model-based feature selection. This is because they provide an easily interpretable measurement, Gini efficiency, and have been found to work great at ranking feature importance in practice.\n\nMeasuring feature importance this way is simple:"},{"metadata":{"_uuid":"eb0242574778edfaed0bc5d0295bba31b1d7a3f9","_cell_guid":"d1f71ff8-1d39-4b82-9c81-4a5281690fe0","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier()\nclf.fit(kepler_X, kepler_y)\n\npd.Series(clf.feature_importances_, index=kepler.columns[1:]).plot.bar(color='steelblue', figsize=(12, 6))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f94a7aaa00c10e1b2255ce7d813bbf94787db03","_cell_guid":"c614275c-8413-4079-a193-4976c7ca0587"},"cell_type":"markdown","source":"These results agree decently well with the results we got from the information-theoretic approach in the previous section. Note: I demonstrated how to build this exact feature importance chart in even *fewer* steps using the `yellowbricks` ML visualization library in [this previous notebook](https://www.kaggle.com/residentmario/ml-visualization-with-yellowbrick-1).\n\n`sklearn` comes with a simple wrapper for turning these feature importance scores into a feature subselection called `SelectFromModel`. With that function in tow, here is the entire process:"},{"metadata":{"_uuid":"6907003e81974e10a0d6243aeebc149bb271a294","_cell_guid":"a8146cea-8c54-4e0d-9136-380516faaae0","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\n\nclf = DecisionTreeClassifier()\ntrans = SelectFromModel(clf, threshold='median')\nkepler_X_trans = trans.fit_transform(kepler_X, kepler_y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"107ef18ba4ba1244ddae3b7bcd8a9aff01e79094","_cell_guid":"a1809577-d3fc-4cd9-b764-42594b0c9a1f","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"We started with {0} features but retained only {1} of them!\".format(kepler_X.shape[1] - 1, kepler_X_trans.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0086daa5da2b48c3fd872227001b74f8ba21acb2","_cell_guid":"9c80ef3d-5d7b-472b-bccc-0801c94ba29f","collapsed":true,"trusted":false},"cell_type":"code","source":"columns_retained_FromMode = kepler.iloc[:, 1:].columns[trans.get_support()].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e951dcd132ae58b4013f728a4d1a99bc39f9946","_cell_guid":"c5eb3779-9d29-4e01-afc8-8d71d8894e8c"},"cell_type":"markdown","source":"It doesn't come equipped with the cool statistical doo-dads that `GenericUnivariateSelect` has access to, but in practice this approach is probably used far more often."},{"metadata":{"_uuid":"6d1d8936ebaf68bbbe564cf49644d8d705fb0796","_cell_guid":"5d5e4da0-b004-4918-81d5-d63a8b53b97d"},"cell_type":"markdown","source":"## Recursive feature elimination (RFE and RFECV)\n\nRecursive feature elimination is the `sklearn` sort-of implementation of a really old idea (stepwise selection) that used to be very popular. Given a set of data, build a model on that data, then assess the importance of the variables and prune the weakest feature. Rebuild the classifier (especially in later steps, the coefficient coverage will change), then repeat the process. Keep doing this until the desired number of features is reached.\n\nThis methodology is implemented in `RFE`, and is basically just a looped alternative to the `SelectFromModel` all-at-once approach. The advantage of this approach is that it will not remove variables which were deemed insignificant at the beginning of the process, but become more and more significant as lesser features are removed. For datasets with many variables relatively strongly correlated with one another and relatively weakly correlated with the target variable, this approach may result in slightly different feature choices from those made by naive model-based selection. The disadvantage is that since you have to train the model many times, this approach is multiplicatively slower than the one-and-done."},{"metadata":{"_uuid":"a23962a88bf705afce323e31f8ef0a3413ca74d2","_cell_guid":"09faeba7-9a18-4ea8-b8d8-0af8f5604141","collapsed":true,"trusted":false},"cell_type":"code","source":"from sklearn.feature_selection import RFE\n\nclf = DecisionTreeClassifier()\ntrans = RFE(clf, n_features_to_select=20)\nkepler_X_trans = trans.fit_transform(kepler_X, kepler_y)\ncolumns_retained_RFE = kepler.iloc[:, 1:].columns[trans.get_support()].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d8d398f9693b42fd3415911719a2ae0d53adc78","_cell_guid":"c1dbc899-515f-4a2e-9b1e-f9efb38ba5b1"},"cell_type":"markdown","source":"`RFECV` is a slight tweak to `RFE` that uses cross-validation to determine the optimal stopping point in terms of numbers of columns (eliminating user choice in the `n_features_to_select` attribute)."},{"metadata":{"_uuid":"53f32d080a790ad88505aacf8ea6f5b3cdd73a62","_cell_guid":"2fca2eef-97de-4070-a846-42f347fec246","collapsed":true,"trusted":false},"cell_type":"code","source":"from sklearn.feature_selection import RFECV\n\nclf = DecisionTreeClassifier()\ntrans = RFECV(clf)\nkepler_X_trans = trans.fit_transform(kepler_X, kepler_y)\ncolumns_retained_RFECV = kepler.iloc[:, 1:].columns[trans.get_support()].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1202af7c2b0250a472cc310e2121bb971236e00","_cell_guid":"4f828d68-cbab-4ac8-ac2e-ed1d5281b98a"},"cell_type":"markdown","source":"It appears that I accidentally chose a good number of predictor variables, at least as far as `RFECV` is concerned:"},{"metadata":{"_uuid":"96d98da921d348c4619e1d82f9a791acdbcd0441","scrolled":true,"_cell_guid":"dd286377-d57e-4481-8a33-d79e34af0c87","trusted":false,"collapsed":true},"cell_type":"code","source":"len(columns_retained_RFECV)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb4d26a3b8d6a6fd2096bec5a09700e1e0fe3369","_cell_guid":"20b90f1f-a0b5-480a-a30e-f056a8368f68"},"cell_type":"markdown","source":"While I won't spend any more time treating RFECV here, you can read a bit more on cross-validation and get some insight into how this algorithm works by looking at [this notebook](https://www.kaggle.com/residentmario/cross-validation-schemes-with-food-consumption/)."},{"metadata":{"_uuid":"f3432fb8c86143539898fa2d66eac825f7796e72","_cell_guid":"e85270a1-9452-4a6a-aa18-3474cf437339"},"cell_type":"markdown","source":"# Conclusion\n\nAlthough the different approaches agree on the most important variables in the dataset, they disagree on the finer points. This results in sometimes different, sometimes very different column selections, depending on the algorithm. The following list considers the 3! possible pairs of feature choices, and the difference in the sets chosen by the selectors for the Kepler dataset:"},{"metadata":{"_uuid":"91d5a9b1ccee3d8635385dbb9b42e87b151618ce","_cell_guid":"c62ad92b-bd89-40ba-9963-d29a016c6078","trusted":false,"collapsed":true},"cell_type":"code","source":"import itertools\npairs = {}\nfor (i, (a, b)) in enumerate(itertools.combinations([set(columns_retained_Select), set(columns_retained_FromMode), set(columns_retained_RFE), set(columns_retained_RFECV)], 2)):\n    pairs.update({str(i): len(a.difference(b))})\n    \nprint(\"Enumerating differences between  3!\")\nlist(pairs.values())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd90d07d6b88b1616640506c0bd8ab89375edcd4","_cell_guid":"74a4fd96-00b0-4f97-8070-50b9ffa63ee8"},"cell_type":"markdown","source":"That these different approaches came to such deviant conclusions is not unexpected; it's mainly a signal that the weaker of the 20 features we've selected mostly come down to noise, as all of the feature importance and ranking visualizations we have constructed so far have shown.\n\nThat concludes this notebook! In the next notebook I will step beyond `sklearn` built-in to consider a feature selection algorithm implemented as an `sklearn-contrib` module: `boruta`.\n\nSee that notebook [here](https://www.kaggle.com/residentmario/automated-feature-selection-with-boruta).\n\nPS: you may also like [this thread](https://www.kaggle.com/questions-and-answers/46335) on the Kaggle Q&A Forum."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}