{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "709e824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the original dataset\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "# Normalize the features\n",
    "norm_data = (data - data.mean()) / data.std()\n",
    "# Split the data into training and testing sets\n",
    "train_data = norm_data.sample(frac=0.8, random_state=42)\n",
    "test_data = norm_data.drop(train_data.index)\n",
    "\n",
    "# Define the GAN architecture\n",
    "def make_generator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=6))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(32))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(6, activation='tanh'))\n",
    "    return model\n",
    "\n",
    "def make_discriminator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=6))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(32))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(6, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Initialize the GAN\n",
    "generator = make_generator_model()\n",
    "discriminator = make_discriminator_model()\n",
    "\n",
    "# Define the loss functions\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# Define the optimizers\n",
    "generator_optimizer = Adam(1e-4)\n",
    "discriminator_optimizer = Adam(1e-4)\n",
    "\n",
    "# Define the training loop\n",
    "@tf.function\n",
    "def train_step(real_data, batch_size, noise_dim):\n",
    "    # Generate noise\n",
    "    noise = tf.random.normal([batch_size, noise_dim])\n",
    "\n",
    "    # Train the discriminator\n",
    "    with tf.GradientTape() as tape:\n",
    "        generated_data = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(real_data, training=True)\n",
    "        fake_output = discriminator(generated_data, training=True)\n",
    "\n",
    "        real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "        fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "        total_loss = real_loss + fake_loss\n",
    "\n",
    "    grads = tape.gradient(total_loss, discriminator.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n",
    "\n",
    "    # Train the generator\n",
    "    with tf.GradientTape() as tape:\n",
    "        generated_data = generator(noise, training=True)\n",
    "        fake_output = discriminator(generated_data, training=True)\n",
    "        gen_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "    grads = tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(grads, generator.trainable_variables))\n",
    "\n",
    "# Train the GAN\n",
    "def train_gan(generator, discriminator, train_data, epochs, batch_size, noise_dim):\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(len(train_data) // batch_size):\n",
    "            real_data = train_data[i * batch_size:(i + 1) * batch_size]\n",
    "            train_step(real_data, batch_size, noise_dim)\n",
    "\n",
    "# Generate synthetic data\n",
    "def generate_synthetic_data(generator, noise_dim, num_samples):\n",
    "    noise = tf.random.normal([num_samples, noise_dim])\n",
    "    generated_data = generator(noise, training=False)\n",
    "    return generated_data\n",
    "\n",
    "# # Train the GAN on the training data\n",
    "# epochs = 1000\n",
    "# batch_size = 32\n",
    "# noise_dim = 32\n",
    "\n",
    "# train_gan(generator, discriminator, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8108ef3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Slmss\\AppData\\Local\\Temp\\ipykernel_3248\\4150616828.py\", line 25, in train_step  *\n        real_output = discriminator(real_data, training=True)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_11\" is incompatible with the layer: expected shape=(None, 6), found shape=(32, 8)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m      4\u001b[0m noise_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Generate synthetic data\u001b[39;00m\n\u001b[0;32m      9\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n",
      "Cell \u001b[1;32mIn[8], line 84\u001b[0m, in \u001b[0;36mtrain_gan\u001b[1;34m(generator, discriminator, train_data, epochs, batch_size, noise_dim)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_data) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size):\n\u001b[0;32m     83\u001b[0m     real_data \u001b[38;5;241m=\u001b[39m train_data[i \u001b[38;5;241m*\u001b[39m batch_size:(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m batch_size]\n\u001b[1;32m---> 84\u001b[0m     \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filehagwad0t.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[1;34m(real_data, batch_size, noise_dim)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m     10\u001b[0m     generated_data \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(generator), (ag__\u001b[38;5;241m.\u001b[39mld(noise),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope)\n\u001b[1;32m---> 11\u001b[0m     real_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(discriminator), (ag__\u001b[38;5;241m.\u001b[39mld(real_data),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope)\n\u001b[0;32m     12\u001b[0m     fake_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(discriminator), (ag__\u001b[38;5;241m.\u001b[39mld(generated_data),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope)\n\u001b[0;32m     13\u001b[0m     real_loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(cross_entropy), (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mones_like, (ag__\u001b[38;5;241m.\u001b[39mld(real_output),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), ag__\u001b[38;5;241m.\u001b[39mld(real_output)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py:295\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m--> 295\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    296\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    297\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    298\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    299\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplay_shape(x\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    300\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Slmss\\AppData\\Local\\Temp\\ipykernel_3248\\4150616828.py\", line 25, in train_step  *\n        real_output = discriminator(real_data, training=True)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_11\" is incompatible with the layer: expected shape=(None, 6), found shape=(32, 8)\n"
     ]
    }
   ],
   "source": [
    "# Train the GAN on the training data\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "noise_dim = 32\n",
    "\n",
    "train_gan(generator, discriminator, train_data, epochs, batch_size, noise_dim)\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 200\n",
    "synthetic_data = generate_synthetic_data(generator, noise_dim, num_samples)\n",
    "\n",
    "# Inverse the normalization\n",
    "synthetic_data = (synthetic_data * data.std()) + data.mean()\n",
    "\n",
    "# Add the synthetic data to the original data\n",
    "augmented_data = pd.concat([data, synthetic_data], ignore_index=True)\n",
    "\n",
    "# Split the augmented data into training and testing sets\n",
    "aug_train_data = augmented_data.sample(frac=0.8, random_state=42)\n",
    "aug_test_data = augmented_data.drop(aug_train_data.index)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
