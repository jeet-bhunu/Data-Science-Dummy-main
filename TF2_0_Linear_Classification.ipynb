{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slmsshk/Data_Science-_Dummy/blob/main/TF2_0_Linear_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeNaBVyT4pn1"
      },
      "source": [
        "# Install TensorFlow\n",
        "# !pip install -q tensorflow-gpu==2.0.0-beta1\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x  # Colab only.\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtMPQxpC4xD0"
      },
      "source": [
        "# Load in the data\n",
        "from sklearn.datasets import load_breast_cancer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwJ9LvEE5B_z"
      },
      "source": [
        "# load the data\n",
        "data = load_breast_cancer()\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNkavJSj5EcV"
      },
      "source": [
        "# check the type of 'data'\n",
        "type(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsWYmnYR5GQq"
      },
      "source": [
        "# note: it is a Bunch object\n",
        "# this basically acts like a dictionary where you can treat the keys like attributes\n",
        "data.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRYJw4hN5JXD"
      },
      "source": [
        "# 'data' (the attribute) means the input data\n",
        "data.data.shape\n",
        "# it has 569 samples, 30 features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5OVVVge5L-h"
      },
      "source": [
        "# 'targets'\n",
        "data.target\n",
        "# note how the targets are just 0s and 1s\n",
        "# normally, when you have K targets, they are labeled 0..K-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFXPFIiB5Osq"
      },
      "source": [
        "# their meaning is not lost\n",
        "data.target_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hujO3u7j5Qm-"
      },
      "source": [
        "# there are also 569 corresponding targets\n",
        "data.target.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3o3h2l95SJ0"
      },
      "source": [
        "# you can also determine the meaning of each feature\n",
        "data.feature_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__iUSucB5Tug"
      },
      "source": [
        "# normally we would put all of our imports at the top\n",
        "# but this lets us tell a story\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# split the data into train and test sets\n",
        "# this lets us simulate how our model will perform in the future\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.33)\n",
        "N, D = X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS7cFmYl73H3"
      },
      "source": [
        "# Scale the data\n",
        "# you'll learn why scaling is needed in a later course\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-llhgXY5bmX"
      },
      "source": [
        "# Now all the fun Tensorflow stuff\n",
        "# Build the model\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Input(shape=(D,)),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        " #Alternatively, you can do:\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(1, input_shape=(D,), activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train the model\n",
        "r = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=11)\n",
        "\n",
        "\n",
        "# Evaluate the model - evaluate() returns loss and accuracy\n",
        "print(\"Train score:\", model.evaluate(X_train, y_train))\n",
        "print(\"Test score:\", model.evaluate(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0fK5PQc8aJ_"
      },
      "source": [
        "# Plot what's returned by model.fit()\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(r.history['loss'], label='loss')\n",
        "plt.plot(r.history['val_loss'], label='val_loss')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBAA2UY28vJN"
      },
      "source": [
        "# Plot the accuracy too\n",
        "plt.plot(r.history['accuracy'], label='acc')\n",
        "plt.plot(r.history['val_accuracy'], label='val_acc')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrZnHBI8NH-u"
      },
      "source": [
        "# Part 2: Making Predictions\n",
        "\n",
        "This goes with the lecture \"Making Predictions\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss1LJwIVKREf"
      },
      "source": [
        "# Make predictions\n",
        "P = model.predict(X_test)\n",
        "print(P) # they are outputs of the sigmoid, interpreted as probabilities p(y = 1 | x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv0cN7J6KoVu"
      },
      "source": [
        "# Round to get the actual predictions\n",
        "# Note: has to be flattened since the targets are size (N,) while the predictions are size (N,1)\n",
        "import numpy as np\n",
        "P = np.round(P).flatten()\n",
        "print(P)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G7oRdnQsjtL"
      },
      "source": [
        "# Calculate the accuracy, compare it to evaluate() output\n",
        "print(\"Manually calculated accuracy:\", np.mean(P == y_test))\n",
        "print(\"Evaluate output:\", model.evaluate(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iogYs0uGNS4l"
      },
      "source": [
        "# Part 3: Saving and Loading a Model\n",
        "\n",
        "This goes with the lecture \"Saving and Loading a Model\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1uHxJ2SNXan"
      },
      "source": [
        "# Let's now save our model to a file\n",
        "model.save('linearclassifier.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6PHmjb0OFBf"
      },
      "source": [
        "# Check that the model file exists\n",
        "!ls -lh "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgFBrBveNoOR"
      },
      "source": [
        "# Let's load the model and confirm that it still works\n",
        "# Note: there is a bug in Keras where load/save only works if you DON'T use the Input() layer explicitly\n",
        "# So, make sure you define the model with ONLY Dense(1, input_shape=(D,))\n",
        "# At least, until the bug is fixed\n",
        "# https://github.com/keras-team/keras/issues/10417\n",
        "model = tf.keras.models.load_model('linearclassifier.h5')\n",
        "print(model.layers)\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXsp_UOXxM9T"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8rGQI-Rpo2T"
      },
      "source": [
        "\n",
        "# Download the file - requires Chrome (at this point)\n",
        "from google.colab import files\n",
        "files.download('linearclassifier.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}